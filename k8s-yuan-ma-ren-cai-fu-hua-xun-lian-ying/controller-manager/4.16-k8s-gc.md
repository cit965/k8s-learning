# 4.16 k8s GC

## 背景 <a href="#gai-shu" id="gai-shu"></a>

设想这么一个场景：我们在 k8s 上创建了一个对象，它根据需要生成副本集和 Pod。在检查时，我们遗漏了容器某个属性的设置，因此又重新编辑了 Deployment。

新的 Deployment 就产生了新的 replicaset 对象和新的 Pod。这里就出现了一个问题，旧的 ReplicaSet 和 Pod 去哪了？另外，如果直接删除 deployment，那 replicaset 和 pod 又会如何？事实就是，在删除 Deployment 后，ReplicaSet 和 Pod 也会一起被删除，要不然集群早就乱套了。

在这个场景之下，我们可以深入思考几个问题：**在 K8s 中该如何实现级联删除？有几种级联删除策略？在 K8s 中有没有可能存在孤儿对象（orphan object）？**&#x8FD9;些问题其实就是典型的垃圾回收（garbage collection，GC）问题。

## 概述 <a href="#gai-shu" id="gai-shu"></a>

一般来说，**垃圾回收（GC）就是从系统中删除不再使用的对象，并释放分配给它们的计算资源。**&#x47;C 存在于所有的高级编程语言中，较低级的编程语言通过系统库实现 GC。

和编程语言中(例如 Go, Java) 的 GC 运行机制一样，Kubernetes 内部也有垃圾回收机制，用于清理集群中的下列资源:

* 终止的 Pod
* 已完成的 Job
* 附属的主对象已经不存在的对象
* 未使用的容器和容器镜像
* StorageClass 回收策略为 Delete 的 PV 卷
* 过期的证书签名
* …

## Owner, Dependent <a href="#owner-dependent" id="owner-dependent"></a>

在面向对象的语言中，一些对象会引用其他对象或者直接由其他对象组成，k8s 也有类似形式，例如 ReplicaSet 管理一组 Pod，而 Deployment 又管理着 ReplicaSet。

但与面向对象语言不同的是，在 K8s 对象的定义中，没有明确所有者之间的关系，那么系统要如何确定它们的关系呢？其实，在 K8s 中，每个从属对象都具有 唯一数据字段名称 `metadata.ownerReferences` 用于确定关系，大多数情况下，k8s 会自动的管理所属者引用。

Kubernetes 中被依赖的资源对象称之为 Owner (属主资源), 依赖其他资源的对象称之为 Dependent (依赖资源), 例如我们创建了一个副本数量为 3 的 Deployment。

```yaml
# 官方示例 controllers/nginx-deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3 # 副本数量，可以根据实际情况修改
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
```

那么会产生如下的依赖关系:

* Pod 作为 ReplicaSet 的 Dependent, 它的 Owner 为 ReplicaSet (Deployment 底层实现需要 ReplicaSet)
* ReplicaSet 作为 Owner 的同时也同样作为 Dependent, 它的 Owner 为 Deployment, Dependent 为 3 个依赖它的 Pod
* Deployment 作为 Owner, 它的 Dependent 为依赖的 ReplicaSet

<figure><img src="../../.gitbook/assets/image (26).png" alt=""><figcaption></figcaption></figure>

### 级联删除 （Cascading deletion） <a href="#cascading-deletion" id="cascading-deletion"></a>

Kubernetes 检查并删除不再具有所有者引用的对象，例如删除 ReplicaSet 时留下的 pod。当你删除一个对象时，你可以控制 Kubernetes 是否自动删除该对象的依赖项，这个过程称&#x4E3A;_&#x7EA7;联删除_。级联删除有两种类型，如下：

* Foreground cascading deletion\
  前台级联删除
* Background cascading deletion\
  后台级联删除

\
您还可以使用 **Finalizers** 控制垃圾回收控制器如何以及何时删除具有所有者引用的资源。

#### Foreground cascading deletion 前台级联删除 <a href="#foreground-deletion" id="foreground-deletion"></a>

在前台级联删除中，您要删除的所有者对象首先进&#x5165;_&#x5220;除进行&#x4E2D;_&#x72B6;态。在这种状态下，所有者对象会发生以下情况：

* Kubernetes API server 将对象的`metadata.deletionTimestamp`字段设置为对象被标记为删除的时间
* Kubernetes API server 将`metadata.finalizers`字段设置为`foregroundDeletion`&#x20;

当所有者对象进入删除进行中状态后，控制器删除依赖项。删除所有依赖对象后，控制器删除所有者对象。此时，该对象在 Kubernetes API 中不再可见。在前台级联删除期间，阻止所有者删除的唯一依赖项是具有`ownerReference.blockOwnerDeletion=true`字段的依赖项。

默认情况下，Kubernetes 使用[后台级联删除](https://kubernetes.io/docs/concepts/architecture/garbage-collection/#background-deletion)来删除对象的依赖项。您可以使用`kubectl`或 Kubernetes API 切换到前台级联删除，命令如下：

<pre class="language-bash"><code class="lang-bash"><strong>// 前台级联删除示例
</strong><strong>kubectl delete deployment nginx-deployment --cascade=foreground
</strong>kubectl proxy --port=8080
curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/deployments/nginx-deployment \
    -d '{"kind":"DeleteOptions","apiVersion":"v1","propagationPolicy":"Foreground"}' \
    -H "Content-Type: application/json"

</code></pre>

在后台级联删除中，Kubernetes API 服务器立即删除所有者对象，控制器在后台清理依赖对象。默认情况下，Kubernetes 使用后台级联删除，除非您手动使用前台删除或选择孤立依赖对象。

### Delete owner objects and orphan dependents 删除所有者对象和孤儿依赖项 <a href="#set-orphan-deletion-policy" id="set-orphan-deletion-policy"></a>

默认情况下，当您告诉 Kubernetes 删除对象时，[控制器](https://kubernetes.io/docs/concepts/architecture/controller/)也会删除依赖对象。您可以使用`kubectl`或 Kubernetes API 使 Kubernete&#x73;_&#x5B64;&#x7ACB;_&#x8FD9;些依赖项。

```go
// 孤儿删除示例
kubectl delete deployment nginx-deployment --cascade=orphan
```

#### Finalizer <a href="#finalizer" id="finalizer"></a>

&#x20;k8s 通过填充`.metadata.deletionTimestamp`来标记要删除的对象，并返回`202`状态代码（HTTP“已接受”）。在完全删除已经标记为删除的资源之前会等待特定条件满足。当控制平面或其他组件执行 finalizers 定义的操作时，目标对象保持终止状态。这些操作完成后，控制器从目标对象中删除相关的finalizers。当`metadata.finalizers`字段为空时，k8s 才认为删除完成并删除该对象。

当我们创建资源时，可以在`metadata.finalizers`字段中指定终结器。当您尝试删除资源时，处理删除请求的 API 服务器会注意到`finalizers`字段中的值并执行以下操作：

* 修改对象以添加`metadata.deletionTimestamp`字段，其中包含开始删除的时间。
* 防止对象被删除，直到从其`metadata.finalizers`字段中删除所有项目
* 返回`202`状态代码（HTTP“已接受”）

终结器的一个常见示例是`kubernetes.io/pv-protection` ，它可以防止意外删除`PersistentVolume`对象。当 Pod 使用`PersistentVolume`对象时，Kubernetes 会添加`pv-protection`终结器。如果您尝试删除`PersistentVolume` ，它会进入`Terminating`状态，但控制器无法删除它，因为终结器存在。当 Pod 停止使用`PersistentVolume`时，Kubernetes 会清除`pv-protection`终结器，并且控制器会删除该卷。

## 源码 <a href="#yuan-ma-shuo-ming" id="yuan-ma-shuo-ming"></a>

本文着重从源代码的角度分析一下 GarbageCollector 的实现原理，GarbageCollector 功能对应的源代码位于 Kubernetes 项目的 `pkg/controller/garbagecollector/` 目录，本文以 Kubernetes `v1.30.0` 版本源代码进行分析。

<figure><img src="../../.gitbook/assets/image (27).png" alt=""><figcaption></figcaption></figure>

### GarbageCollector 架构图 <a href="#garbagecollector" id="garbagecollector"></a>

<figure><img src="../../.gitbook/assets/image (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

<pre class="language-go"><code class="lang-go"><strong>// GarbageCollector 对象表示垃圾回收控制器，是实现垃圾回收功能的核心对象。
</strong><strong>// 通过 reflector 监视受管API对象的更改，将结果汇集到单线程的dependencyGraphBuilder中
</strong><strong>// dependencyGraphBuilder 构建了一张 DAG 图，缓存对象之间依赖关系
</strong><strong>// 当图发生变化时，将对象放入 attemptToDelete 和 attemptToOrphan 队列
</strong><strong>type GarbageCollector struct {
</strong>	...
	
	// attemptToDelete 队列
	// 存储垃圾回收尝试删除的资源对象
	attemptToDelete workqueue.RateLimitingInterface
	// attemptToOrphan 队列
	// 存储垃圾回收尝试删除的资源对象所依赖的对象
	attemptToOrphan        workqueue.RateLimitingInterface
	// 资源对象 DAG 构造器 
	dependencyGraphBuilder *GraphBuilder
}
</code></pre>

#### **1个图**

（1）`uidToNode`：对象关联依赖关系图，由`GraphBuilder`维护，维护着所有对象间的关联依赖关系。在该图里，每一个k8s对象会对应着关系图里的一个`node`，而每个`node`都会维护一个`owner`列表以及`dependent`列表。

示例：现有一个deployment A，replicaset B（owner为deployment A），pod C（owner为replicaset B），则对象关联依赖关系如下：

```
3个node，分别是A、B、C

A对应一个node，无owner，dependent列表里有B；  
B对应一个node，owner列表里有A，dependent列表里有C；  
C对应一个node，owner列表里有B，无dependent。  
```

<figure><img src="../../.gitbook/assets/image (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

#### **2个处理器**

（1）`GraphBuilder`：负责维护所有对象的关联依赖关系图，并产生事件触发`GarbageCollector`执行对象回收删除操作。`GraphBuilder`从`graphChanges`事件队列中获取事件进行消费，根据资源对象中`ownerReference`的值，来构建、更新、删除对象间的关联依赖关系图，也即`owner`与`dependent`之间的关系图，然后再作为生产者生产事件，放入`attemptToDelete`或`attemptToOrphan`队列中，触发`GarbageCollector`执行，看是否需要进行关联对象的回收删除操作，而`GarbageCollector`进行对象的回收删除操作时会依赖于`uidToNode`这个关系图。

（2）`GarbageCollector`：负责回收删除对象。`GarbageCollector`作为消费者，从`attemptToDelete`与`attemptToOrphan`队列中取出事件进行处理，若一个对象被删除，且其删除策略为级联删除，则进行关联对象的回收删除。关于删除关联对象，细一点说就是，使用级联删除策略去删除一个`owner`时，会连带这个`owner`对象的`dependent`对象也一起删除掉。



#### **3个事件队列**

（1）`graphChanges`：list/watch apiserver，获取事件，由`informer`生产，由`GraphBuilder`消费；

（2）`attemptToDelete`：级联删除事件队列，由`GraphBuilder`生产，由`GarbageCollector`消费；

（3）`attemptToOrphan`：孤儿删除事件队列，由`GraphBuilder`生产，由`GarbageCollector`消费。



### 初始化 <a href="#chu-shi-hua" id="chu-shi-hua"></a>

`NewGarbageCollector` 方法初始化`GraphBuilder`结构体，并赋值给`GarbageCollector`结构体的`dependencyGraphBuilder`属性

```go
func NewGarbageCollector(...) (*GarbageCollector, error) {
// NewGarbageCollector creates a new GarbageCollector.
func NewGarbageCollector(
	ctx context.Context,
	kubeClient clientset.Interface,
	metadataClient metadata.Interface,
	mapper meta.ResettableRESTMapper,
	ignoredResources map[schema.GroupResource]struct{},
	sharedInformers informerfactory.InformerFactory,
	informersStarted <-chan struct{},
) (*GarbageCollector, error) {
	graphBuilder := NewDependencyGraphBuilder(ctx, metadataClient, mapper, ignoredResources, sharedInformers, informersStarted)
	return NewComposedGarbageCollector(ctx, kubeClient, metadataClient, mapper, graphBuilder)
}
```

### 启动入口 <a href="#qi-dong-ru-kou" id="qi-dong-ru-kou"></a>

垃圾回收的启动入口位于 Kubernetes 项目的 `/cmd/kube-controller-manager/app/core.go` 文件中。

```go
func startGarbageCollectorController(ctx context.Context, ...) (controller.Interface, bool, error) {
	// 初始化 NewGarbageCollector 对象需要的各项参数
	
	...

	// 设置垃圾回收需要忽视的资源类型
	ignoredResources := make(map[schema.GroupResource]struct{})
	for _, r := range controllerContext.ComponentConfig.GarbageCollectorController.GCIgnoredResources {
		ignoredResources[schema.GroupResource{Group: r.Group, Resource: r.Resource}] = struct{}{}
	}
	
	// 创建一个 NewGarbageCollector 对象
	garbageCollector, err := garbagecollector.NewGarbageCollector(
        ...
	)
	
	// 获取垃圾回收并发的 goroutine 数量 (默认为 20 个)
	workers := int(controllerContext.ComponentConfig.GarbageCollectorController.ConcurrentGCSyncs)
	// 启动垃圾回收
	go garbageCollector.Run(ctx, workers)

	// 监听集群内的资源对象变化并同步需要被删除的资源对象
	go garbageCollector.Sync(ctx, discoveryClient, 30*time.Second)

	return garbageCollector, true, nil
}
```

从上面的源代码可以看到，启动垃圾回收器时，调用的核心方法为 `GarbageCollector.Run` 和 `GarbageCollector.Sync`。

### Run

&#x20;`GarbageCollector.Run` 方法作为垃圾回收器的入口方法，主要做两件事情:

1. 单独启动一个 goroutine 执行资源对象的 DAG 图构造和同步
2. 根据配置启动相应数量的 goroutine 来处理存储将被回收的资源对象的 `attemptToDelete` 队列和`attemptToOrphan` 队列

```go
func (gc *GarbageCollector) Run(ctx context.Context, workers int) {
	...
	
    // 启动一个 goroutine 执行 DAG 的构建
    // gc.dependencyGraphBuilder.Run 负责启动启动 GraphBuilder，主要逻辑如下：
		//（1）调用 gb.startMonitors，启动 infomers；
		//（2）每隔1s循环调用 gb.runProcessGraphChanges，做GraphBuilder 的核心逻辑处理
	go gc.dependencyGraphBuilder.Run(ctx)

	// 等待所有资源对象的 DAG 构建完成
	if !cache.WaitForNamedCacheSync("garbage collector", ctx.Done(), func() bool {
		return gc.dependencyGraphBuilder.IsSynced(logger)
	}) {
		return
	}
	
	// 所有准备工作就绪之后，就可以执行垃圾回收了
	
	// 根据配置启动多个 goroutine 来执行垃圾回收
	for i := 0; i < workers; i++ {
		// 负责处理要被删除的对象，从 attemptToDelete 队列中获取 item 做相应处理
		go wait.UntilWithContext(ctx, gc.runAttemptToDeleteWorker, 1*time.Second)
		// 负责处理要被孤立的对象，从 attemptToOrphan 队列中获取 node ，找到其 dependents 对象，修改 orphan finalizer
		go wait.Until(func() { gc.runAttemptToOrphanWorker(logger) }, 1*time.Second, ctx.Done())
	}

	<-ctx.Done()
}
```

### Sync

周期性的查询集群中所有的`deletableResources`，调用`gc.resyncMonitors`来更新`GraphBuilder`的`monitors`，为新出现的资源对象初始化`infomer`和注册`eventHandler`，然后启动`infomer`，对已经移除的资源对象的`monitors`进行销毁。

```go
func (gc *GarbageCollector) Sync(ctx context.Context, ...) {
	    // 获取可以被回收的资源对象
		newResources, err := GetDeletableResources(logger, discoveryClient)
		
		...

		// 检测可回收对象是否发生变化
		// 如果没有任何对象发生变化，意味着本轮垃圾回收无需执行
		if reflect.DeepEqual(oldResources, newResources) {
			return
		}

        // 代码执行到这里，说明需要进行一轮垃圾回收操作
        // 因为垃圾回收过程中涉及到重建资源 DAG
        // 所以需要加锁，暂停异步执行的 goroutine
        gc.workerLock.Lock()
        defer gc.workerLock.Unlock()

		attempt := 0
		wait.PollImmediateUntilWithContext(ctx, 100*time.Millisecond, func(ctx context.Context) (bool, error) {
			attempt++

			// 每一轮, 重新获取可以被回收的资源对象
			if attempt > 1 {
				newResources, err = GetDeletableResources(logger, discoveryClient)
				
				...
			}
			
			// 重置 REST mapper
			gc.restMapper.Reset()
			
			// 同步 resource monitors
			if err := gc.resyncMonitors(logger, newResources); err != nil {
				return false, nil
			}

			// 等待所有资源对象的 DAG 构建完成
			if !cache.WaitForNamedCacheSync("garbage collector", waitForStopOrTimeout(ctx.Done(), period), func() bool {
				return gc.dependencyGraphBuilder.IsSynced(logger)
			}) {
                ...
				return false, nil
			}
			
			return true, nil
		})

		// 更换新旧资源对象
		oldResources = newResources
	}, period)
}
```

#### gc.resyncMonitors

调用`gc.dependencyGraphBuilder.syncMonitors`：初始化`infomer`和注册`eventHandler`；\
调用`gc.dependencyGraphBuilder.startMonitors`：启动`infomer`。

```go
// resyncMonitors starts or stops resource monitors as needed to ensure that all
// (and only) those resources present in the map are monitored.
func (gc *GarbageCollector) resyncMonitors(logger klog.Logger, deletableResources map[schema.GroupVersionResource]struct{}) error {
	if err := gc.dependencyGraphBuilder.syncMonitors(logger, deletableResources); err != nil {
		return err
	}
	gc.dependencyGraphBuilder.startMonitors(logger)
	return nil
}
```

### NewDebugHandler

garbagecollector.NewDebugHandler暴露http服务，注册 debug 接口，用于debug，用来提供由`GraphBuilder`构建的集群内所有对象的关联关系。

获取全部的对象关联关系图：

```css
curl http://{master_ip}:{kcm_port}/debug/controllers/garbagecollector/graph -o {output_file}
```

获取特定uid的对象关联关系图：

```bash
curl http://{master_ip}:{kcm_port}/debug/controllers/garbagecollector/graph?uid={project_uid} -o {output_file}
```

示例：

```bash
curl http://192.168.1.10:10252/debug/controllers/garbagecollector/graph?uid=8727f640-112e-21eb-11dd-626400510df6 -o /home/test
```

```go
func (h *debugHTTPHandler) ServeHTTP(w http.ResponseWriter, req *http.Request) {
	if req.URL.Path != "/graph" {
		http.Error(w, "", http.StatusNotFound)
		return
	}

	var nodes []*dotVertex
	var edges []dotEdge
	if uidStrings := req.URL.Query()["uid"]; len(uidStrings) > 0 {
		uids := []types.UID{}
		for _, uidString := range uidStrings {
			uids = append(uids, types.UID(uidString))
		}
		nodes, edges = h.controller.dependencyGraphBuilder.uidToNode.ToDOTNodesAndEdgesForObj(uids...)

	} else {
		nodes, edges = h.controller.dependencyGraphBuilder.uidToNode.ToDOTNodesAndEdges()
	}

	b := bytes.NewBuffer(nil)
	if err := marshalDOT(b, nodes, edges); err != nil {
		http.Error(w, err.Error(), http.StatusInternalServerError)
		return
	}

	w.Header().Set("Content-Type", "text/vnd.graphviz")
	w.Header().Set("X-Content-Type-Options", "nosniff")
	w.Write(b.Bytes())
	w.WriteHeader(http.StatusOK)
}

func (gc *GarbageCollector) DebuggingHandler() http.Handler {
	return NewDebugHandler(gc)
}

```

## 详细分析

垃圾回收控制器中最关键的代码就是`garbagecollector.go`与`graph_builder.go`两部分，也即`GarbageCollector struct`与`GraphBuilder struct`，所以下面处理逻辑分析将分成两大块进行分析。

### 1.GraphBuilder

GraphBuilder 主要有2个功能：

1. 基于 informers 中的资源事件在 `uidToNode` 属性中维护着所有对象的关联依赖关系
2. 处理 `graphChanges` 中的事件，并作为生产者将事件放入到 `attemptToDelete` 和 `attemptToOrphan` 两个队列中，触发消费者`GarbageCollector`进行对象的回收删除操作。

先来简单的分析下`GraphBuilder struct`，里面最关键的几个属性及作用如下：

1. `graphChanges`：informers 监听到的事件会放在 `graphChanges` 中，然后`GraphBuilder`会作为消费者，处理`graphChanges`队列中的事件
2. `uidToNode`（对象依赖关联关系图）：根据对象uid，维护所有对象的关联依赖关系，也即前面说的`owner`与`dependent`之间的关系，也可以理解为`GraphBuilder`会维护一张所有对象的关联依赖关系图
3. `attemptToDelete`与`attemptToOrphan`：`GraphBuilder`作为生产者往`attemptToDelete` 和 `attemptToOrphan` 两个队列中存放事件，然后`GarbageCollector`作为消费者会处理两个队列中的事件。

```go
// GraphBuilder processes events supplied by the informers, updates uidToNode,
// a graph that caches the dependencies as we know, and enqueues
// items to the attemptToDelete and attemptToOrphan.
type GraphBuilder struct {
	restMapper meta.RESTMapper

	// each monitor list/watches a resource, the results are funneled to the
	// dependencyGraphBuilder
	monitors    monitors
	monitorLock sync.RWMutex
	// informersStarted is closed after after all of the controllers have been initialized and are running.
	// After that it is safe to start them here, before that it is not.
	informersStarted <-chan struct{}

	// stopCh drives shutdown. When a receive from it unblocks, monitors will shut down.
	// This channel is also protected by monitorLock.
	stopCh <-chan struct{}

	// running tracks whether Run() has been called.
	// it is protected by monitorLock.
	running bool

	eventRecorder    record.EventRecorder
	eventBroadcaster record.EventBroadcaster

	metadataClient metadata.Interface
	// monitors are the producer of the graphChanges queue, graphBuilder alters
	// the in-memory graph according to the changes.
	graphChanges workqueue.TypedRateLimitingInterface[*event]
	// uidToNode doesn't require a lock to protect, because only the
	// single-threaded GraphBuilder.processGraphChanges() reads/writes it.
	uidToNode *concurrentUIDToNode
	// GraphBuilder is the producer of attemptToDelete and attemptToOrphan, GC is the consumer.
	attemptToDelete workqueue.TypedRateLimitingInterface[*node]
	attemptToOrphan workqueue.TypedRateLimitingInterface[*node]
	// GraphBuilder and GC share the absentOwnerCache. Objects that are known to
	// be non-existent are added to the cached.
	absentOwnerCache *ReferenceCache
	sharedInformers  informerfactory.InformerFactory
	ignoredResources map[schema.GroupResource]struct{}
}
type concurrentUIDToNode struct {
	uidToNodeLock sync.RWMutex
	uidToNode     map[types.UID]*node
}

// 从结构体定义中可以看到，一个k8s对象对应着对象关联依赖关系图里的一个node
// 而每个node都会维护一个owner列表以及dependent列表。
type node struct {
	identity objectReference
	// dependents will be read by the orphan() routine, we need to protect it with a lock.
	dependentsLock sync.RWMutex
	// dependents are the nodes that have node.identity as a
	// metadata.ownerReference.
	dependents map[*node]struct{}
	// this is set by processGraphChanges() if the object has non-nil DeletionTimestamp
	// and has the FinalizerDeleteDependents.
	deletingDependents     bool
	deletingDependentsLock sync.RWMutex
	// this records if the object's deletionTimestamp is non-nil.
	beingDeleted     bool
	beingDeletedLock sync.RWMutex
	// this records if the object was constructed virtually and never observed via informer event
	virtual     bool
	virtualLock sync.RWMutex
	// when processing an Update event, we need to compare the updated
	// ownerReferences with the owners recorded in the graph.
	owners []metav1.OwnerReference
}
```

<figure><img src="../../.gitbook/assets/截屏2024-08-11 14.14.27.png" alt=""><figcaption></figcaption></figure>

接下来看到`GraphBuilder`的处理逻辑部分，Run方法是入口，核心函数有两个，分别是 startMonitors  和 runProcessGraphChanges。

#### startMonitors()

```go
// startMonitors ensures the current set of monitors are running. Any newly
// started monitors will also cause shared informers to be started.
//
// If called before Run, startMonitors does nothing (as there is no stop channel
// to support monitor/informer execution).
func (gb *GraphBuilder) startMonitors(logger klog.Logger) {
	gb.monitorLock.Lock()
	defer gb.monitorLock.Unlock()

	if !gb.running {
		return
	}

	// we're waiting until after the informer start that happens once all the controllers are initialized.  This ensures
	// that they don't get unexpected events on their work queues.
	<-gb.informersStarted

	monitors := gb.monitors
	started := 0
	for _, monitor := range monitors {
		if monitor.stopCh == nil {
			monitor.stopCh = make(chan struct{})
			gb.sharedInformers.Start(gb.stopCh)
			go monitor.Run()
			started++
		}
	}
	logger.V(4).Info("started new monitors", "new", started, "current", len(monitors))

```

#### runProcessGraphChanges()

前面说过监听到的事件会放入到 `graphChanges` 队列中，然后`GraphBuilder`会作为消费者，处理`graphChanges`队列中的事件，而`processGraphChanges`方法就是`GraphBuilder`作为消费者处理`graphChanges`队列中事件地方。

所以在此方法中，`GraphBuilder`既是消费者又是生产者，消费处理`graphChanges` 中的所有事件并进行分类，再生产事件放入到 `attemptToDelete` 和 `attemptToOrphan` 两个队列中去，让`GarbageCollector`作为消费者去处理这两个队列中的事件。

主要逻辑：

* 从`graphChanges`队列中取出事件进行处理
* 读取`uidToNode`，判断该对象是否已经存在于已构建的对象依赖关联关系图中，根据对象是否存在于对象依赖关联关系图中以及事件类型来做不同的处理逻辑；
* 若 `uidToNode` 中不存在该 `node` 且该事件是 `addEvent` 或 `updateEvent`，则为该 `object` 创建对应的 `node`，并调用 `gb.insertNode` 将该 `node` 加到 `uidToNode` 中，然后将该 `node` 添加到其 `owner` 的 `dependents` 中。然后再调用 `gb.processTransitions` 方法做处理，该方法的处理逻辑是判断该对象是否处于删除状态，若处于删除状态会判断该对象是以 `orphan` 模式删除还是以 `foreground` 模式删除（其实就是判断deployment对象的finalizer来区分删除模式，删除deployment的时候会带上删除策略，kube-apiserver会根据删除策略给deployment对象打上相应的finalizer），若以 `orphan` 模式删除，则将该 `node` 加入到 `attemptToOrphan` 队列中，若以 `foreground` 模式删除则将该对象以及其所有 `dependents` 都加入到 `attemptToDelete` 队列中；
* 若 `uidToNode` 中存在该 `node` 且该事件是 `addEvent` 或 `updateEvent` 时，则调用 `referencesDiffs` 方法检查该对象的 `OwnerReferences` 字段是否有变化，有变化则做相应处理，更新对象依赖关联关系图，最后调用 `gb.processTransitions`做处理；
* 若事件为删除事件，则调用`gb.removeNode`，从`uidToNode`中删除该对象，然后从该`node`所有`owners`的`dependents`中删除该对象，再把该对象的`dependents`放入到`attemptToDelete`队列中，触发`GarbageCollector`处理；最后检查该 `node` 的所有 `owners`，若有处于删除状态的 `owner`，此时该 `owner` 可能处于删除阻塞状态正在等待该 `node` 的删除，将该 `owner` 加入到 `attemptToDelete`队列中，触发`GarbageCollector`处理。

```go
// Dequeueing an event from graphChanges, updating graph, populating dirty_queue.
func (gb *GraphBuilder) processGraphChanges(logger klog.Logger) bool {
	item, quit := gb.graphChanges.Get()
	if quit {
		return false
	}
	defer gb.graphChanges.Done(item)
	event := item
	obj := item.obj
	accessor, err := meta.Accessor(obj)
	if err != nil {
		utilruntime.HandleError(fmt.Errorf("cannot access obj: %v", err))
		return true
	}

	logger.V(5).Info("GraphBuilder process object",
		"apiVersion", event.gvk.GroupVersion().String(),
		"kind", event.gvk.Kind,
		"object", klog.KObj(accessor),
		"uid", string(accessor.GetUID()),
		"eventType", event.eventType,
		"virtual", event.virtual,
	)

	// Check if the node already exists
	existingNode, found := gb.uidToNode.Read(accessor.GetUID())
	if found && !event.virtual && !existingNode.isObserved() {
		// this marks the node as having been observed via an informer event
		// 1. this depends on graphChanges only containing add/update events from the actual informer
		// 2. this allows things tracking virtual nodes' existence to stop polling and rely on informer events
		observedIdentity := identityFromEvent(event, accessor)
		if observedIdentity != existingNode.identity {
			// find dependents that don't match the identity we observed
			_, potentiallyInvalidDependents := partitionDependents(existingNode.getDependents(), observedIdentity)
			// add those potentially invalid dependents to the attemptToDelete queue.
			// if their owners are still solid the attemptToDelete will be a no-op.
			// this covers the bad child -> good parent observation sequence.
			// the good parent -> bad child observation sequence is handled in addDependentToOwners
			for _, dep := range potentiallyInvalidDependents {
				if len(observedIdentity.Namespace) > 0 && dep.identity.Namespace != observedIdentity.Namespace {
					// Namespace mismatch, this is definitely wrong
					logger.V(2).Info("item references an owner but does not match namespaces",
						"item", dep.identity,
						"owner", observedIdentity,
					)
					gb.reportInvalidNamespaceOwnerRef(dep, observedIdentity.UID)
				}
				gb.attemptToDelete.Add(dep)
			}

			// make a copy (so we don't modify the existing node in place), store the observed identity, and replace the virtual node
			logger.V(2).Info("replacing virtual item with observed item",
				"virtual", existingNode.identity,
				"observed", observedIdentity,
			)
			existingNode = existingNode.clone()
			existingNode.identity = observedIdentity
			gb.uidToNode.Write(existingNode)
		}
		existingNode.markObserved()
	}
	switch {
	case (event.eventType == addEvent || event.eventType == updateEvent) && !found:
		newNode := &node{
			identity:           identityFromEvent(event, accessor),
			dependents:         make(map[*node]struct{}),
			owners:             accessor.GetOwnerReferences(),
			deletingDependents: beingDeleted(accessor) && hasDeleteDependentsFinalizer(accessor),
			beingDeleted:       beingDeleted(accessor),
		}
		gb.insertNode(logger, newNode)
		// the underlying delta_fifo may combine a creation and a deletion into
		// one event, so we need to further process the event.
		gb.processTransitions(logger, event.oldObj, accessor, newNode)
	case (event.eventType == addEvent || event.eventType == updateEvent) && found:
		// handle changes in ownerReferences
		added, removed, changed := referencesDiffs(existingNode.owners, accessor.GetOwnerReferences())
		if len(added) != 0 || len(removed) != 0 || len(changed) != 0 {
			// check if the changed dependency graph unblock owners that are
			// waiting for the deletion of their dependents.
			gb.addUnblockedOwnersToDeleteQueue(logger, removed, changed)
			// update the node itself
			existingNode.owners = accessor.GetOwnerReferences()
			// Add the node to its new owners' dependent lists.
			gb.addDependentToOwners(logger, existingNode, added)
			// remove the node from the dependent list of node that are no longer in
			// the node's owners list.
			gb.removeDependentFromOwners(existingNode, removed)
		}

		if beingDeleted(accessor) {
			existingNode.markBeingDeleted()
		}
		gb.processTransitions(logger, event.oldObj, accessor, existingNode)
	case event.eventType == deleteEvent:
		if !found {
			logger.V(5).Info("item doesn't exist in the graph, this shouldn't happen",
				"item", accessor.GetUID(),
			)
			return true
		}

		removeExistingNode := true

		if event.virtual {
			// this is a virtual delete event, not one observed from an informer
			deletedIdentity := identityFromEvent(event, accessor)
			if existingNode.virtual {

				// our existing node is also virtual, we're not sure of its coordinates.
				// see if any dependents reference this owner with coordinates other than the one we got a virtual delete event for.
				if matchingDependents, nonmatchingDependents := partitionDependents(existingNode.getDependents(), deletedIdentity); len(nonmatchingDependents) > 0 {

					// some of our dependents disagree on our coordinates, so do not remove the existing virtual node from the graph
					removeExistingNode = false

					if len(matchingDependents) > 0 {
						// mark the observed deleted identity as absent
						gb.absentOwnerCache.Add(deletedIdentity)
						// attempt to delete dependents that do match the verified deleted identity
						for _, dep := range matchingDependents {
							gb.attemptToDelete.Add(dep)
						}
					}

					// if the delete event verified existingNode.identity doesn't exist...
					if existingNode.identity == deletedIdentity {
						// find an alternative identity our nonmatching dependents refer to us by
						replacementIdentity := getAlternateOwnerIdentity(nonmatchingDependents, deletedIdentity)
						if replacementIdentity != nil {
							// replace the existing virtual node with a new one with one of our other potential identities
							replacementNode := existingNode.clone()
							replacementNode.identity = *replacementIdentity
							gb.uidToNode.Write(replacementNode)
							// and add the new virtual node back to the attemptToDelete queue
							gb.attemptToDelete.AddRateLimited(replacementNode)
						}
					}
				}

			} else if existingNode.identity != deletedIdentity {
				// do not remove the existing real node from the graph based on a virtual delete event
				removeExistingNode = false

				// our existing node which was observed via informer disagrees with the virtual delete event's coordinates
				matchingDependents, _ := partitionDependents(existingNode.getDependents(), deletedIdentity)

				if len(matchingDependents) > 0 {
					// mark the observed deleted identity as absent
					gb.absentOwnerCache.Add(deletedIdentity)
					// attempt to delete dependents that do match the verified deleted identity
					for _, dep := range matchingDependents {
						gb.attemptToDelete.Add(dep)
					}
				}
			}
		}

		if removeExistingNode {
			// removeNode updates the graph
			gb.removeNode(existingNode)
			existingNode.dependentsLock.RLock()
			defer existingNode.dependentsLock.RUnlock()
			if len(existingNode.dependents) > 0 {
				gb.absentOwnerCache.Add(identityFromEvent(event, accessor))
			}
			for dep := range existingNode.dependents {
				gb.attemptToDelete.Add(dep)
			}
			for _, owner := range existingNode.owners {
				ownerNode, found := gb.uidToNode.Read(owner.UID)
				if !found || !ownerNode.isDeletingDependents() {
					continue
				}
				// this is to let attempToDeleteItem check if all the owner's
				// dependents are deleted, if so, the owner will be deleted.
				gb.attemptToDelete.Add(ownerNode)
			}
		}
	}
	return true
}
```

#### insertNode()

调用 `gb.insertNode` 将 `node` 加到 `uidToNode` 中，然后将该 `node` 添加到其 `owner` 的 `dependents` 中。

```go
// insertNode insert the node to gb.uidToNode; then it finds all owners as listed
// in n.owners, and adds the node to their dependents list.
func (gb *GraphBuilder) insertNode(logger klog.Logger, n *node) {
	gb.uidToNode.Write(n)
	gb.addDependentToOwners(logger, n, n.owners)
}
```

#### processTransitions()

gb.processTransitions 方法检查k8s对象是否处于删除状态（对象的`deletionTimestamp`属性不为空则处于删除状态），并且对象里含有删除策略对应的`finalizer`，然后做相应的处理。

因为只有删除策略为`Foreground`或`Orphan`时对象才会会设置相关`Finalizer`，所以该方法只会处理删除策略为`Foreground`或`Orphan`的对象，对于删除策略为`Background`的对象不做处理。

若对象的`deletionTimestamp`属性不为空，且有`Orphaned`删除策略对应的`finalizer`，则将对应的`node`放入到 `attemptToOrphan` 队列中，触发`GarbageCollector`去消费处理；

若对象的`deletionTimestamp`属性不为空，且有`foreground`删除策略对应的`finalizer`，则调用`n.markDeletingDependents`标记 `node`的 `deletingDependents` 属性为 `true`，代表该`node`的`dependents`正在被删除，并将对应的`node`及其`dependents`放入到 `attemptToDelete` 队列中，触发`GarbageCollector`去消费处理。

```go
func (gb *GraphBuilder) processTransitions(logger klog.Logger, oldObj interface{}, newAccessor metav1.Object, n *node) {
	if startsWaitingForDependentsOrphaned(oldObj, newAccessor) {
		logger.V(5).Info("add item to attemptToOrphan", "item", n.identity)
		gb.attemptToOrphan.Add(n)
		return
	}
	if startsWaitingForDependentsDeleted(oldObj, newAccessor) {
		logger.V(2).Info("add item to attemptToDelete, because it's waiting for its dependents to be deleted", "item", n.identity)
		// if the n is added as a "virtual" node, its deletingDependents field is not properly set, so always set it here.
		n.markDeletingDependents()
		for dep := range n.dependents {
			gb.attemptToDelete.Add(dep)
		}
		gb.attemptToDelete.Add(n)
	}
}
```

#### removeNode()

调用`gb.removeNode`，从`uidToNode`中删除该对象，然后从该`node`所有`owners`的`dependents`中删除该对象，再把该对象的`dependents`放入到`attemptToDelete`队列中，触发`GarbageCollector`处理；最后检查该 `node` 的所有 `owners`，若有处于删除状态的 `owner`，此时该 `owner` 可能处于删除阻塞状态正在等待该 `node` 的删除，将该 `owner` 加入到 `attemptToDelete`队列中，触发`GarbageCollector`处理。

```go
// removeNode removes the node from gb.uidToNode, then finds all
// owners as listed in n.owners, and removes n from their dependents list.
func (gb *GraphBuilder) removeNode(n *node) {
	gb.uidToNode.Delete(n.identity.UID)
	gb.removeDependentFromOwners(n, n.owners)
}
// removeDependentFromOwners remove n from owners' dependents list.
func (gb *GraphBuilder) removeDependentFromOwners(n *node, owners []metav1.OwnerReference) {
	for _, owner := range owners {
		ownerNode, ok := gb.uidToNode.Read(owner.UID)
		if !ok {
			continue
		}
		ownerNode.deleteDependent(n)
	}
}


```

### 2.GarbageCollector

```go
type GarbageCollector struct {
	restMapper     meta.ResettableRESTMapper
	metadataClient metadata.Interface
	// garbage collector attempts to delete the items in attemptToDelete queue when the time is ripe.
	attemptToDelete workqueue.TypedRateLimitingInterface[*node]
	// garbage collector attempts to orphan the dependents of the items in the attemptToOrphan queue, then deletes the items.
	attemptToOrphan        workqueue.TypedRateLimitingInterface[*node]
	dependencyGraphBuilder *GraphBuilder
	// GC caches the owners that do not exist according to the API server.
	absentOwnerCache *ReferenceCache

	kubeClient       clientset.Interface
	eventBroadcaster record.EventBroadcaster

	workerLock sync.RWMutex
}
```

GarbageCollector 主要有2个功能：

1. 处理 `attemptToDelete`队列中的事件，根据对象删除策略`foreground`或`background`做相应的回收逻辑处理，删除关联对象；
2. 处理 `attemptToOrphan`队列中的事件，根据对象删除策略`Orphan`，更新该`owner`的所有`dependents`对象，将对象的`OwnerReferences`属性中该`owner`的相关字段去除，接着再更新该`owner`对象，去除`Orphan`删除策略对应的`finalizers`。

GarbageCollector的2个关键处理方法：

* `gc.runAttemptToDeleteWorker`：主要负责处理`attemptToDelete`队列中的事件，负责删除策略为`foreground`或`background`的对象回收处理；
* `gc.runAttemptToOrphanWorker`：主要负责处理`attemptToOrphan`队列中的事件，负责删除策略为`Orphan`的对象回收处理。

**runAttemptToDeleteWorker()**

* 从`attemptToDelete`队列中取出对象
* 调用 `gc.attemptToDeleteItem` 尝试删除 `node`
* 若删除失败则重新加入到 `attemptToDelete` 队列中进行重试

```go
func (gc *GarbageCollector) runAttemptToDeleteWorker(ctx context.Context) {
	for gc.processAttemptToDeleteWorker(ctx) {
	}
}

func (gc *GarbageCollector) processAttemptToDeleteWorker(ctx context.Context) bool {
	item, quit := gc.attemptToDelete.Get()
	gc.workerLock.RLock()
	defer gc.workerLock.RUnlock()
	if quit {
		return false
	}
	defer gc.attemptToDelete.Done(item)

	action := gc.attemptToDeleteWorker(ctx, item)
	switch action {
	case forgetItem:
		gc.attemptToDelete.Forget(item)
	case requeueItem:
		gc.attemptToDelete.AddRateLimited(item)
	}

	return true
}
```

**attemptToDeleteItem**

* 判断 `node` 是否处于删除状态
* 从 `apiserver` 获取该 `node` 对应的对象
* 调用`item.isDeletingDependents`方法：通过 `node` 的 `deletingDependents` 字段判断该 `node` 当前是否正在删除 `dependents`，若是则调用 `gc.processDeletingDependentsItem` 方法对`dependents`做进一步处理：检查该`node` 的 `blockingDependents` 是否被完全删除，若是则移除该 `node`对应对象的相关 `finalizer`，若否，则将未删除的 `blockingDependents` 加入到 `attemptToDelete`队列中；上面分析`GraphBuilder`时说到，在 `GraphBuilder` 处理 `graphChanges` 中的事件时，在`processTransitions`方法逻辑里，会调用`n.markDeletingDependents`，标记 `node`的 `deletingDependents` 属性为 `true`
* 调用`gc.classifyReferences`将 `node` 的`owner`分为3类，分别是`solid`、`dangling、waitingForDependentsDeletio`
* 接下来将根据`solid`、`dangling`与`waitingForDependentsDeletion`的数量做不同的逻辑处理；
* 第一种情况：当`solid`数量不为0时，即该`node`至少有一个 `owner` 存在且不处于删除状态，则说明该对象还不能被回收删除，此时将 `dangling` 和 `waitingForDependentsDeletion` 列表中的 `owner` 从 `node` 的 `ownerReferences` 中删除；
* 第二种情况：`solid`数量为0，该 `node` 的 `owner` 处于 `waitingForDependentsDeletion` 状态并且 `node` 的 `dependents` 未被完全删除，将使用`foreground`前台删除策略来删除该`node`对应的对象；
* 当不满足以上两种情况时（即），进入该默认处理逻辑：按照删除对象时使用的删除策略，调用 `apiserver` 的接口删除对象。

```go
func (gc *GarbageCollector) attemptToDeleteWorker(ctx context.Context, item interface{}) workQueueItemAction {
	n, ok := item.(*node)
	if !ok {
		utilruntime.HandleError(fmt.Errorf("expect *node, got %#v", item))
		return forgetItem
	}

	logger := klog.FromContext(ctx)

	if !n.isObserved() {
		nodeFromGraph, existsInGraph := gc.dependencyGraphBuilder.uidToNode.Read(n.identity.UID)
		if !existsInGraph {
			// this can happen if attemptToDelete loops on a requeued virtual node because attemptToDeleteItem returned an error,
			// and in the meantime a deletion of the real object associated with that uid was observed
			logger.V(5).Info("item no longer in the graph, skipping attemptToDeleteItem", "item", n.identity)
			return forgetItem
		}
		if nodeFromGraph.isObserved() {
			// this can happen if attemptToDelete loops on a requeued virtual node because attemptToDeleteItem returned an error,
			// and in the meantime the real object associated with that uid was observed
			logger.V(5).Info("item no longer virtual in the graph, skipping attemptToDeleteItem on virtual node", "item", n.identity)
			return forgetItem
		}
	}

	err := gc.attemptToDeleteItem(ctx, n)
	if err == enqueuedVirtualDeleteEventErr {
		// a virtual event was produced and will be handled by processGraphChanges, no need to requeue this node
		return forgetItem
	} else if err == namespacedOwnerOfClusterScopedObjectErr {
		// a cluster-scoped object referring to a namespaced owner is an error that will not resolve on retry, no need to requeue this node
		return forgetItem
	} else if err != nil {
		if _, ok := err.(*restMappingError); ok {
			// There are at least two ways this can happen:
			// 1. The reference is to an object of a custom type that has not yet been
			//    recognized by gc.restMapper (this is a transient error).
			// 2. The reference is to an invalid group/version. We don't currently
			//    have a way to distinguish this from a valid type we will recognize
			//    after the next discovery sync.
			// For now, record the error and retry.
			logger.V(5).Error(err, "error syncing item", "item", n.identity)
		} else {
			utilruntime.HandleError(fmt.Errorf("error syncing item %s: %v", n, err))
		}
		// retry if garbage collection of an object failed.
		return requeueItem
	} else if !n.isObserved() {
		// requeue if item hasn't been observed via an informer event yet.
		// otherwise a virtual node for an item added AND removed during watch reestablishment can get stuck in the graph and never removed.
		// see https://issue.k8s.io/56121
		logger.V(5).Info("item hasn't been observed via informer yet", "item", n.identity)
		return requeueItem
	}

	return forgetItem
}
```

#### blockingDependents

item.blockingDependents返回会阻塞`node`删除的`dependents`。一个`dependents`会不会阻塞`owner`的删除，主要看这个`dependents`的`ownerReferences`的`blockOwnerDeletion`属性值是否为`true`，为`true`则代表该`dependents`会阻塞`owner`的删除。

```go
// blockingDependents returns the dependents that are blocking the deletion of
// n, i.e., the dependent that has an ownerReference pointing to n, and
// the BlockOwnerDeletion field of that ownerReference is true.
// Note that this function does not provide any synchronization guarantees;
// items could be added to or removed from ownerNode.dependents the moment this
// function returns.
func (n *node) blockingDependents() []*node {
    dependents := n.getDependents()
    var ret []*node
    for _, dep := range dependents {
       for _, owner := range dep.owners {
          if owner.UID == n.identity.UID && owner.BlockOwnerDeletion != nil && *owner.BlockOwnerDeletion {
             ret = append(ret, dep)
          }
       }
    }
    return ret
}
```

**runAttemptToOrphanWorker**

gc.runAttemptToOrphanWorker方法是负责处理`orphan`删除策略删除的 `node`。

gc.runAttemptToDeleteWorker主要逻辑为循环调用`gc.attemptToDeleteWorker`方法。

下面来看一下`gc.attemptToDeleteWorker`方法的主要逻辑：\
（1）从`attemptToOrphan`队列中取出对象；\
（2）调用`gc.orphanDependents`方法：更新该`owner`的所有`dependents`对象，将对象的`OwnerReferences`属性中该`owner`的相关字段去除，失败则将该`owner`重新加入到`attemptToOrphan`队列中；\
（3）调用`gc.removeFinalizer`方法：更新该`owner`对象，去除`Orphan`删除策略对应的`finalizers`。

```go
func (gc *GarbageCollector) runAttemptToOrphanWorker(logger klog.Logger) {
	for gc.processAttemptToOrphanWorker(logger) {
	}
}
// processAttemptToOrphanWorker dequeues a node from the attemptToOrphan, then finds its
// dependents based on the graph maintained by the GC, then removes it from the
// OwnerReferences of its dependents, and finally updates the owner to remove
// the "Orphan" finalizer. The node is added back into the attemptToOrphan if any of
// these steps fail.
func (gc *GarbageCollector) processAttemptToOrphanWorker(logger klog.Logger) bool {
	item, quit := gc.attemptToOrphan.Get()
	gc.workerLock.RLock()
	defer gc.workerLock.RUnlock()
	if quit {
		return false
	}
	defer gc.attemptToOrphan.Done(item)

	action := gc.attemptToOrphanWorker(logger, item)
	switch action {
	case forgetItem:
		gc.attemptToOrphan.Forget(item)
	case requeueItem:
		gc.attemptToOrphan.AddRateLimited(item)
	}

	return true
}
```

**orphanDependents**

主要逻辑：更新指定`owner`的所有`dependents`对象，将对象的`OwnerReferences`属性中该`owner`的相关字段去除，对于每个`dependents`，分别起一个goroutine来处理，加快处理速度。

```go

// dependents are copies of pointers to the owner's dependents, they don't need to be locked.
func (gc *GarbageCollector) orphanDependents(logger klog.Logger, owner objectReference, dependents []*node) error {
	errCh := make(chan error, len(dependents))
	wg := sync.WaitGroup{}
	wg.Add(len(dependents))
	for i := range dependents {
		go func(dependent *node) {
			defer wg.Done()
			// the dependent.identity.UID is used as precondition
			p, err := c.GenerateDeleteOwnerRefStrategicMergeBytes(dependent.identity.UID, []types.UID{owner.UID})
			if err != nil {
				errCh <- fmt.Errorf("orphaning %s failed, %v", dependent.identity, err)
				return
			}
			_, err = gc.patch(dependent, p, func(n *node) ([]byte, error) {
				return gc.deleteOwnerRefJSONMergePatch(n, owner.UID)
			})
			// note that if the target ownerReference doesn't exist in the
			// dependent, strategic merge patch will NOT return an error.
			if err != nil && !errors.IsNotFound(err) {
				errCh <- fmt.Errorf("orphaning %s failed, %v", dependent.identity, err)
			}
		}(dependents[i])
	}
	wg.Wait()
	close(errCh)

	var errorsSlice []error
	for e := range errCh {
		errorsSlice = append(errorsSlice, e)
	}

	if len(errorsSlice) != 0 {
		return fmt.Errorf("failed to orphan dependents of owner %s, got errors: %s", owner, utilerrors.NewAggregate(errorsSlice).Error())
	}
	logger.V(5).Info("successfully updated all dependents", "owner", owner)
	return nil
}

```
