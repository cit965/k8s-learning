# 4.16 k8s GC

## 背景 <a href="#gai-shu" id="gai-shu"></a>

设想这么一个场景：我们在 K8s 上创建了一个对象，它根据需要生成副本集和 Pod。在检查时，我们遗漏了容器某个属性的设置，因此又重新编辑了 Deployment。新的 Deployment 就产生了新的副本集对象和新的 Pod。这里就出现了一个问题，旧的副本集和 Pop 去哪了？另外，如果直接删除 Deployment，那副本集和 Pod 又会如何？事实就是，在删除 Deployment 后，副本集和 Pod 也会一起被删除，要不然集群早就乱套了。

在这个场景之下，我们可以深入思考几个问题：**在 K8s 中该如何实现级联删除？有几种级联删除策略？在 K8s 中有没有可能存在孤儿对象（orphan object）？**这些问题其实就是典型的垃圾回收（garbage collection，GC）问题。

一般来说，**垃圾回收（GC）就是从系统中删除未使用的对象，并释放分配给它们的计算资源。**GC 存在于所有的高级编程语言中，较低级的编程语言通过系统库实现 GC。

## 概述 <a href="#gai-shu" id="gai-shu"></a>

Kubernetes 和内置垃圾回收编程语言 (例如 Go, Java) 一样，内部也有垃圾回收机制，用于清理集群中的下列资源:

* 终止的 Pod
* 已完成的 Job
* 附属的主对象已经不存在的对象
* 未使用的容器和容器镜像
* StorageClass 回收策略为 Delete 的 PV 卷
* 过期的证书签名
* …

和编程语言中的 GC 运行机制一样，Kubernetes 中的垃圾回收周期自动执行，集群内每个运行 kubelet 的节点上都会有一个垃圾回收器在运行， 可以简单将其理解为一个独立运行的进程甚至一个 goroutine, 事实上，Kubernetes 的垃圾回收是以 **控制器资源** 的形式存在和运行的。

## Owner, Dependent <a href="#owner-dependent" id="owner-dependent"></a>

在面向对象的语言中，一些对象会引用其他对象或者直接由其他对象组成，k8s 也有类似形式，例如副本集管理一组 Pod，而 Deployment 又管理着副本集。

但与面向对象语言不同的是，在 K8s 对象的定义中，没有明确所有者之间的关系，那么系统要如何确定它们的关系呢？其实，在 K8s 中，每个从属对象都具有 唯一数据字段名称 `metadata.ownerReferences` 用于确定关系。

Kubernetes 中被依赖的资源对象称之为 Owner (属主资源), 依赖其他资源的对象称之为 Dependent (依赖资源), 例如我们创建了一个副本数量为 3 的 Deployment。

```yaml
# 官方示例 controllers/nginx-deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3 # 副本数量，可以根据实际情况修改
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
```

那么会产生如下的依赖关系:

* Pod 作为 ReplicaSet 的 Dependent, 它的 Owner 为 ReplicaSet (Deployment 底层实现需要 ReplicaSet)
* ReplicaSet 作为 Owner 的同时也同样作为 Dependent, 它的 Owner 为 Deployment, Dependent 为 3 个依赖它的 Pod
* Deployment 作为 Owner, 它的 Dependent 为依赖的 ReplicaSet

<figure><img src="../../.gitbook/assets/image (26).png" alt=""><figcaption></figcaption></figure>

#### 回收机制 <a href="#hui-shou-ji-zhi" id="hui-shou-ji-zhi"></a>

默认情况下，垃圾回收采用的是级联删除机制，例如删除 ReplicaSet 资源对象 R1 之后，会删除依赖 R1 资源对象的 Pod 资源对象， 级联删除有两种类型: **前台级联删除** 和 **后台级联删除**。

**前台级联删除:** 当资源对象进入垃圾回收过程，垃圾回收控制器先删除其全部依赖 (Dependent) 对象，然后删除该资源 (Owner) 对象。

**后台级联删除:** API Server 立即删除该资源 (Owner) 对象，然后由垃圾回收控制器在后台清理其全部依赖 (Dependent) 对象，这是 Kubernetes 默认使用的级联删除方案。

#### 孤儿资源对象 <a href="#gu-er-zi-yuan-dui-xiang" id="gu-er-zi-yuan-dui-xiang"></a>

当 Kubernetes 删除某个资源 (Owner) 对象时，其全部依赖 (Dependent) 对象被称作被遗弃的 (Orphaned) 孤儿资源对象，该功能由下文中的 **Finalizer** 来实现。

#### Finalizer <a href="#finalizer" id="finalizer"></a>

Finalizer 用于防止误操作删除了集群依赖的正常运行资源，Finalizer 可以作为资源对象的属性和资源进行绑定，用于执行资源对象在删除之前的逻辑， 所有对象在删除之前，其 Finalizer 属性字段必须为 `nil`, API Server 才会删除该对象，这样就可以防止级联删除了。

例如，现在试图删除一个正在被多个 Pod 使用的 PersistentVolume (持久卷) 资源，那么该 PersistentVolume 资源不会被理解删除， 因为 PersistentVolume 资源注册了 Finalizer 。

除了防止级联删除之外，还可以在资源对象删除之前执行指定的钩子函数，例如在一些场景中只想删除当前资源对象，而不想级联删除其依赖对象， 这时就可以在该资源对象上注册 OrphanFinalizer, 那么垃圾回收控制器在删除该资源对象之后，会忽略其依赖对象，这样给开发者自定义实现提供了更强的灵活性。

## 源码 <a href="#yuan-ma-shuo-ming" id="yuan-ma-shuo-ming"></a>

本文着重从源代码的角度分析一下 GarbageCollector 的实现原理，GarbageCollector 功能对应的源代码位于 Kubernetes 项目的 `pkg/controller/garbagecollector/` 目录，本文以 Kubernetes `v1.30.0` 版本源代码进行分析。

<figure><img src="../../.gitbook/assets/image (27).png" alt=""><figcaption></figcaption></figure>

### GarbageCollector <a href="#garbagecollector" id="garbagecollector"></a>

<pre class="language-go"><code class="lang-go"><strong>// GarbageCollector 对象表示垃圾回收控制器，是实现垃圾回收功能的核心对象。
</strong><strong>type GarbageCollector struct {
</strong>	...
	
	// attemptToDelete 队列
	// 存储垃圾回收尝试删除的资源对象
	attemptToDelete workqueue.RateLimitingInterface
	// attemptToOrphan 队列
	// 存储垃圾回收尝试删除的资源对象所依赖的对象
	attemptToOrphan        workqueue.RateLimitingInterface
	// 资源对象 DAG 构造器 
	dependencyGraphBuilder *GraphBuilder
}
</code></pre>

通过监听资源变化并将结果构造为 DAG (有向无环图) 结构，DAG 对象存储了集群中不同资源对象之间的从属关系，当 DAG 发生变化时， 对应的资源对象可能会被垃圾回收加入到 `attemptToDelete` 队列，并将对象依赖的对象加入到 `attemptToOrphan` 队列。

<figure><img src="../../.gitbook/assets/image.png" alt=""><figcaption></figcaption></figure>

garbage collector中最关键的代码就是`garbagecollector.go`与`graph_builder.go`两部分。

garbage collector的主要组成为1个图（对象关联依赖关系图）、2个处理器（`GraphBuilder`与`GarbageCollector`）、3个事件队列（`graphChanges`、`attemptToDelete`与`attemptToOrphan`）：

#### **1个图**

（1）`uidToNode`：对象关联依赖关系图，由`GraphBuilder`维护，维护着所有对象间的关联依赖关系。在该图里，每一个k8s对象会对应着关系图里的一个`node`，而每个`node`都会维护一个`owner`列表以及`dependent`列表。

示例：现有一个deployment A，replicaset B（owner为deployment A），pod C（owner为replicaset B），则对象关联依赖关系如下：

```
3个node，分别是A、B、C

A对应一个node，无owner，dependent列表里有B；  
B对应一个node，owner列表里有A，dependent列表里有C；  
C对应一个node，owner列表里有B，无dependent。  
```

<figure><img src="../../.gitbook/assets/image (1).png" alt=""><figcaption></figcaption></figure>

#### **2个处理器**

（1）`GraphBuilder`：负责维护所有对象的关联依赖关系图，并产生事件触发`GarbageCollector`执行对象回收删除操作。`GraphBuilder`从`graphChanges`事件队列中获取事件进行消费，根据资源对象中`ownerReference`的值，来构建、更新、删除对象间的关联依赖关系图，也即`owner`与`dependent`之间的关系图，然后再作为生产者生产事件，放入`attemptToDelete`或`attemptToOrphan`队列中，触发`GarbageCollector`执行，看是否需要进行关联对象的回收删除操作，而`GarbageCollector`进行对象的回收删除操作时会依赖于`uidToNode`这个关系图。

（2）`GarbageCollector`：负责回收删除对象。`GarbageCollector`作为消费者，从`attemptToDelete`与`attemptToOrphan`队列中取出事件进行处理，若一个对象被删除，且其删除策略为级联删除，则进行关联对象的回收删除。关于删除关联对象，细一点说就是，使用级联删除策略去删除一个`owner`时，会连带这个`owner`对象的`dependent`对象也一起删除掉。



#### **3个事件队列**

（1）`graphChanges`：list/watch apiserver，获取事件，由`informer`生产，由`GraphBuilder`消费；

（2）`attemptToDelete`：级联删除事件队列，由`GraphBuilder`生产，由`GarbageCollector`消费；

（3）`attemptToOrphan`：孤儿删除事件队列，由`GraphBuilder`生产，由`GarbageCollector`消费。



### 初始化 <a href="#chu-shi-hua" id="chu-shi-hua"></a>

`NewGarbageCollector` 方法初始化`GraphBuilder`结构体，并赋值给`GarbageCollector`结构体的`dependencyGraphBuilder`属性

```go
func NewGarbageCollector(...) (*GarbageCollector, error) {
// NewGarbageCollector creates a new GarbageCollector.
func NewGarbageCollector(
	ctx context.Context,
	kubeClient clientset.Interface,
	metadataClient metadata.Interface,
	mapper meta.ResettableRESTMapper,
	ignoredResources map[schema.GroupResource]struct{},
	sharedInformers informerfactory.InformerFactory,
	informersStarted <-chan struct{},
) (*GarbageCollector, error) {
	graphBuilder := NewDependencyGraphBuilder(ctx, metadataClient, mapper, ignoredResources, sharedInformers, informersStarted)
	return NewComposedGarbageCollector(ctx, kubeClient, metadataClient, mapper, graphBuilder)
}
```

### 启动入口 <a href="#qi-dong-ru-kou" id="qi-dong-ru-kou"></a>

垃圾回收的启动入口位于 Kubernetes 项目的 `/cmd/kube-controller-manager/app/core.go` 文件中。

```go
func startGarbageCollectorController(ctx context.Context, ...) (controller.Interface, bool, error) {
	// 初始化 NewGarbageCollector 对象需要的各项参数
	
	...

	// 设置垃圾回收需要忽视的资源类型
	ignoredResources := make(map[schema.GroupResource]struct{})
	for _, r := range controllerContext.ComponentConfig.GarbageCollectorController.GCIgnoredResources {
		ignoredResources[schema.GroupResource{Group: r.Group, Resource: r.Resource}] = struct{}{}
	}
	
	// 创建一个 NewGarbageCollector 对象
	garbageCollector, err := garbagecollector.NewGarbageCollector(
        ...
	)
	
	// 获取垃圾回收并发的 goroutine 数量 (默认为 20 个)
	workers := int(controllerContext.ComponentConfig.GarbageCollectorController.ConcurrentGCSyncs)
	// 启动垃圾回收
	go garbageCollector.Run(ctx, workers)

	// 监听集群内的资源对象变化并同步需要被删除的资源对象
	go garbageCollector.Sync(ctx, discoveryClient, 30*time.Second)

	return garbageCollector, true, nil
}
```

从上面的源代码可以看到，启动垃圾回收器时，调用的核心方法为 `GarbageCollector.Run` 和 `GarbageCollector.Sync`。

### Run

&#x20;`GarbageCollector.Run` 方法作为垃圾回收器的入口方法，主要做两件事情:

1. 单独启动一个 goroutine 执行资源对象的 DAG 构造和同步
2. 根据配置启动相应数量的 goroutine 来处理存储将被回收的资源对象的 `attemptToDelete` 队列和`attemptToOrphan` 队列

```go
func (gc *GarbageCollector) Run(ctx context.Context, workers int) {
	...
	
        // 启动一个 goroutine 执行 DAG 的构建
        // gc.dependencyGraphBuilder.Run负责启动启动GraphBuilder，主要逻辑如下：
	//（1）调用gb.startMonitors，启动 infomers；
	//（2）每隔1s循环调用gb.runProcessGraphChanges，做GraphBuilder的核心逻辑处理
	go gc.dependencyGraphBuilder.Run(ctx)

	// 等待所有资源对象的 DAG 构建完成
	if !cache.WaitForNamedCacheSync("garbage collector", ctx.Done(), func() bool {
		return gc.dependencyGraphBuilder.IsSynced(logger)
	}) {
		return
	}
	
	// 所有准备工作就绪之后，就可以执行垃圾回收了
	
	// 根据配置启动多个 goroutine 来执行垃圾回收
	for i := 0; i < workers; i++ {
		// 负责回收队列中要被删除的对象，内部是一个无限循环，通过调用 processAttemptToDeleteWorker 方法来决定退出的具体条件
		go wait.UntilWithContext(ctx, gc.runAttemptToDeleteWorker, 1*time.Second)
		go wait.Until(func() { gc.runAttemptToOrphanWorker(logger) }, 1*time.Second, ctx.Done())
	}

	<-ctx.Done()
}
```

### Sync

周期性的查询集群中所有的`deletableResources`，调用`gc.resyncMonitors`来更新`GraphBuilder`的`monitors`，为新出现的资源对象初始化`infomer`和注册`eventHandler`，然后启动`infomer`，对已经移除的资源对象的`monitors`进行销毁。

```go
func (gc *GarbageCollector) Sync(ctx context.Context, ...) {
	    // 获取可以被回收的资源对象
		newResources, err := GetDeletableResources(logger, discoveryClient)
		
		...

		// 检测可回收对象是否发生变化
		// 如果没有任何对象发生变化，意味着本轮垃圾回收无需执行
		if reflect.DeepEqual(oldResources, newResources) {
			return
		}

        // 代码执行到这里，说明需要进行一轮垃圾回收操作
        // 因为垃圾回收过程中涉及到重建资源 DAG
        // 所以需要加锁，暂停异步执行的 goroutine
        gc.workerLock.Lock()
        defer gc.workerLock.Unlock()

		attempt := 0
		wait.PollImmediateUntilWithContext(ctx, 100*time.Millisecond, func(ctx context.Context) (bool, error) {
			attempt++

			// 每一轮, 重新获取可以被回收的资源对象
			if attempt > 1 {
				newResources, err = GetDeletableResources(logger, discoveryClient)
				
				...
			}
			
			// 重置 REST mapper
			gc.restMapper.Reset()
			
			// 同步 resource monitors
			if err := gc.resyncMonitors(logger, newResources); err != nil {
				return false, nil
			}

			// 等待所有资源对象的 DAG 构建完成
			if !cache.WaitForNamedCacheSync("garbage collector", waitForStopOrTimeout(ctx.Done(), period), func() bool {
				return gc.dependencyGraphBuilder.IsSynced(logger)
			}) {
                ...
				return false, nil
			}
			
			return true, nil
		})

		// 更换新旧资源对象
		oldResources = newResources
	}, period)
}
```

#### gc.resyncMonitors

调用`gc.dependencyGraphBuilder.syncMonitors`：初始化`infomer`和注册`eventHandler`；\
调用`gc.dependencyGraphBuilder.startMonitors`：启动`infomer`。

```go
// resyncMonitors starts or stops resource monitors as needed to ensure that all
// (and only) those resources present in the map are monitored.
func (gc *GarbageCollector) resyncMonitors(logger klog.Logger, deletableResources map[schema.GroupVersionResource]struct{}) error {
	if err := gc.dependencyGraphBuilder.syncMonitors(logger, deletableResources); err != nil {
		return err
	}
	gc.dependencyGraphBuilder.startMonitors(logger)
	return nil
}
```

### NewDebugHandler

garbagecollector.NewDebugHandler暴露http服务，注册 debug 接口，用于debug，用来提供由`GraphBuilder`构建的集群内所有对象的关联关系。

获取全部的对象关联关系图：

```css
curl http://{master_ip}:{kcm_port}/debug/controllers/garbagecollector/graph -o {output_file}
```

获取特定uid的对象关联关系图：

```bash
curl http://{master_ip}:{kcm_port}/debug/controllers/garbagecollector/graph?uid={project_uid} -o {output_file}
```

示例：

```bash
curl http://192.168.1.10:10252/debug/controllers/garbagecollector/graph?uid=8727f640-112e-21eb-11dd-626400510df6 -o /home/test
```

```go
func (h *debugHTTPHandler) ServeHTTP(w http.ResponseWriter, req *http.Request) {
	if req.URL.Path != "/graph" {
		http.Error(w, "", http.StatusNotFound)
		return
	}

	var nodes []*dotVertex
	var edges []dotEdge
	if uidStrings := req.URL.Query()["uid"]; len(uidStrings) > 0 {
		uids := []types.UID{}
		for _, uidString := range uidStrings {
			uids = append(uids, types.UID(uidString))
		}
		nodes, edges = h.controller.dependencyGraphBuilder.uidToNode.ToDOTNodesAndEdgesForObj(uids...)

	} else {
		nodes, edges = h.controller.dependencyGraphBuilder.uidToNode.ToDOTNodesAndEdges()
	}

	b := bytes.NewBuffer(nil)
	if err := marshalDOT(b, nodes, edges); err != nil {
		http.Error(w, err.Error(), http.StatusInternalServerError)
		return
	}

	w.Header().Set("Content-Type", "text/vnd.graphviz")
	w.Header().Set("X-Content-Type-Options", "nosniff")
	w.Write(b.Bytes())
	w.WriteHeader(http.StatusOK)
}

func (gc *GarbageCollector) DebuggingHandler() http.Handler {
	return NewDebugHandler(gc)
}

```

## 详细分析

前面讲过，`garbage collector`中最关键的代码就是`garbagecollector.go`与`graph_builder.go`两部分，也即`GarbageCollector struct`与`GraphBuilder struct`，所以下面处理逻辑分析将分成两大块进行分析。

### 1.GraphBuilder

GraphBuilder 主要有2个功能：\
（1）基于 informers 中的资源事件在 `uidToNode` 属性中维护着所有对象的关联依赖关系；\
（2）处理 `graphChanges` 中的事件，并作为生产者将事件放入到 `attemptToDelete` 和 `attemptToOrphan` 两个队列中，触发消费者`GarbageCollector`进行对象的回收删除操作。

#### 1.1  结构体

先来简单的分析下`GraphBuilder struct`，里面最关键的几个属性及作用如下：\
（1）`graphChanges`：informers 监听到的事件会放在 `graphChanges` 中，然后`GraphBuilder`会作为消费者，处理`graphChanges`队列中的事件；\
（2）`uidToNode`（对象依赖关联关系图）：根据对象uid，维护所有对象的关联依赖关系，也即前面说的`owner`与`dependent`之间的关系，也可以理解为`GraphBuilder`会维护一张所有对象的关联依赖关系图，而`GarbageCollector`进行对象的回收删除操作时会依赖于这个关系图；\
（3）`attemptToDelete`与`attemptToOrphan`：`GraphBuilder`作为生产者往`attemptToDelete` 和 `attemptToOrphan` 两个队列中存放事件，然后`GarbageCollector`作为消费者会处理 `attemptToDelete` 和 `attemptToOrphan` 两个队列中的事件。

```go
// GraphBuilder processes events supplied by the informers, updates uidToNode,
// a graph that caches the dependencies as we know, and enqueues
// items to the attemptToDelete and attemptToOrphan.
type GraphBuilder struct {
	restMapper meta.RESTMapper

	// each monitor list/watches a resource, the results are funneled to the
	// dependencyGraphBuilder
	monitors    monitors
	monitorLock sync.RWMutex
	// informersStarted is closed after after all of the controllers have been initialized and are running.
	// After that it is safe to start them here, before that it is not.
	informersStarted <-chan struct{}

	// stopCh drives shutdown. When a receive from it unblocks, monitors will shut down.
	// This channel is also protected by monitorLock.
	stopCh <-chan struct{}

	// running tracks whether Run() has been called.
	// it is protected by monitorLock.
	running bool

	eventRecorder    record.EventRecorder
	eventBroadcaster record.EventBroadcaster

	metadataClient metadata.Interface
	// monitors are the producer of the graphChanges queue, graphBuilder alters
	// the in-memory graph according to the changes.
	graphChanges workqueue.TypedRateLimitingInterface[*event]
	// uidToNode doesn't require a lock to protect, because only the
	// single-threaded GraphBuilder.processGraphChanges() reads/writes it.
	uidToNode *concurrentUIDToNode
	// GraphBuilder is the producer of attemptToDelete and attemptToOrphan, GC is the consumer.
	attemptToDelete workqueue.TypedRateLimitingInterface[*node]
	attemptToOrphan workqueue.TypedRateLimitingInterface[*node]
	// GraphBuilder and GC share the absentOwnerCache. Objects that are known to
	// be non-existent are added to the cached.
	absentOwnerCache *ReferenceCache
	sharedInformers  informerfactory.InformerFactory
	ignoredResources map[schema.GroupResource]struct{}
}
type concurrentUIDToNode struct {
	uidToNodeLock sync.RWMutex
	uidToNode     map[types.UID]*node
}
type node struct {
	identity objectReference
	// dependents will be read by the orphan() routine, we need to protect it with a lock.
	dependentsLock sync.RWMutex
	// dependents are the nodes that have node.identity as a
	// metadata.ownerReference.
	dependents map[*node]struct{}
	// this is set by processGraphChanges() if the object has non-nil DeletionTimestamp
	// and has the FinalizerDeleteDependents.
	deletingDependents     bool
	deletingDependentsLock sync.RWMutex
	// this records if the object's deletionTimestamp is non-nil.
	beingDeleted     bool
	beingDeletedLock sync.RWMutex
	// this records if the object was constructed virtually and never observed via informer event
	virtual     bool
	virtualLock sync.RWMutex
	// when processing an Update event, we need to compare the updated
	// ownerReferences with the owners recorded in the graph.
	owners []metav1.OwnerReference
}
```

从结构体定义中可以看到，一个k8s对象对应着对象关联依赖关系图里的一个`node`，而每个`node`都会维护一`个owner`列表以及`dependent`列表。

#### **1.2 GraphBuilder-gb.processGraphChanges**

接下来看到`GraphBuilder`的处理逻辑部分，从`gb.processGraphChanges`作为入口进行处理逻辑分析。

前面说过，informers 监听到的事件会放入到 `graphChanges` 队列中，然后`GraphBuilder`会作为消费者，处理`graphChanges`队列中的事件，而`processGraphChanges`方法就是`GraphBuilder`作为消费者处理`graphChanges`队列中事件地方。

所以在此方法中，`GraphBuilder`既是消费者又是生产者，消费处理`graphChanges` 中的所有事件并进行分类，再生产事件放入到 `attemptToDelete` 和 `attemptToOrphan` 两个队列中去，让`GarbageCollector`作为消费者去处理这两个队列中的事件。

主要逻辑：\
（1）从`graphChanges`队列中取出事件进行处理；

（2）读取`uidToNode`，判断该对象是否已经存在于已构建的对象依赖关联关系图中；下面就开始根据对象是否存在于对象依赖关联关系图中以及事件类型来做不同的处理逻辑；

（3）若 `uidToNode` 中不存在该 `node` 且该事件是 `addEvent` 或 `updateEvent`，则为该 `object` 创建对应的 `node`，并调用 `gb.insertNode` 将该 `node` 加到 `uidToNode` 中，然后将该 `node` 添加到其 `owner` 的 `dependents` 中；\
然后再调用 `gb.processTransitions` 方法做处理，该方法的处理逻辑是判断该对象是否处于删除状态，若处于删除状态会判断该对象是以 `orphan` 模式删除还是以 `foreground` 模式删除（其实就是判断deployment对象的finalizer来区分删除模式，删除deployment的时候会带上删除策略，kube-apiserver会根据删除策略给deployment对象打上相应的finalizer），若以 `orphan` 模式删除，则将该 `node` 加入到 `attemptToOrphan` 队列中，若以 `foreground` 模式删除则将该对象以及其所有 `dependents` 都加入到 `attemptToDelete` 队列中；

（4）若 `uidToNode` 中存在该 `node` 且该事件是 `addEvent` 或 `updateEvent` 时，则调用 `referencesDiffs` 方法检查该对象的 `OwnerReferences` 字段是否有变化，有变化则做相应处理，更新对象依赖关联关系图，最后调用 `gb.processTransitions`做处理；

（5）若事件为删除事件，则调用`gb.removeNode`，从`uidToNode`中删除该对象，然后从该`node`所有`owners`的`dependents`中删除该对象，再把该对象的`dependents`放入到`attemptToDelete`队列中，触发`GarbageCollector`处理；最后检查该 `node` 的所有 `owners`，若有处于删除状态的 `owner`，此时该 `owner` 可能处于删除阻塞状态正在等待该 `node` 的删除，将该 `owner` 加入到 `attemptToDelete`队列中，触发`GarbageCollector`处理。

```go
// Dequeueing an event from graphChanges, updating graph, populating dirty_queue.
func (gb *GraphBuilder) processGraphChanges(logger klog.Logger) bool {
	item, quit := gb.graphChanges.Get()
	if quit {
		return false
	}
	defer gb.graphChanges.Done(item)
	event := item
	obj := item.obj
	accessor, err := meta.Accessor(obj)
	if err != nil {
		utilruntime.HandleError(fmt.Errorf("cannot access obj: %v", err))
		return true
	}

	logger.V(5).Info("GraphBuilder process object",
		"apiVersion", event.gvk.GroupVersion().String(),
		"kind", event.gvk.Kind,
		"object", klog.KObj(accessor),
		"uid", string(accessor.GetUID()),
		"eventType", event.eventType,
		"virtual", event.virtual,
	)

	// Check if the node already exists
	existingNode, found := gb.uidToNode.Read(accessor.GetUID())
	if found && !event.virtual && !existingNode.isObserved() {
		// this marks the node as having been observed via an informer event
		// 1. this depends on graphChanges only containing add/update events from the actual informer
		// 2. this allows things tracking virtual nodes' existence to stop polling and rely on informer events
		observedIdentity := identityFromEvent(event, accessor)
		if observedIdentity != existingNode.identity {
			// find dependents that don't match the identity we observed
			_, potentiallyInvalidDependents := partitionDependents(existingNode.getDependents(), observedIdentity)
			// add those potentially invalid dependents to the attemptToDelete queue.
			// if their owners are still solid the attemptToDelete will be a no-op.
			// this covers the bad child -> good parent observation sequence.
			// the good parent -> bad child observation sequence is handled in addDependentToOwners
			for _, dep := range potentiallyInvalidDependents {
				if len(observedIdentity.Namespace) > 0 && dep.identity.Namespace != observedIdentity.Namespace {
					// Namespace mismatch, this is definitely wrong
					logger.V(2).Info("item references an owner but does not match namespaces",
						"item", dep.identity,
						"owner", observedIdentity,
					)
					gb.reportInvalidNamespaceOwnerRef(dep, observedIdentity.UID)
				}
				gb.attemptToDelete.Add(dep)
			}

			// make a copy (so we don't modify the existing node in place), store the observed identity, and replace the virtual node
			logger.V(2).Info("replacing virtual item with observed item",
				"virtual", existingNode.identity,
				"observed", observedIdentity,
			)
			existingNode = existingNode.clone()
			existingNode.identity = observedIdentity
			gb.uidToNode.Write(existingNode)
		}
		existingNode.markObserved()
	}
	switch {
	case (event.eventType == addEvent || event.eventType == updateEvent) && !found:
		newNode := &node{
			identity:           identityFromEvent(event, accessor),
			dependents:         make(map[*node]struct{}),
			owners:             accessor.GetOwnerReferences(),
			deletingDependents: beingDeleted(accessor) && hasDeleteDependentsFinalizer(accessor),
			beingDeleted:       beingDeleted(accessor),
		}
		gb.insertNode(logger, newNode)
		// the underlying delta_fifo may combine a creation and a deletion into
		// one event, so we need to further process the event.
		gb.processTransitions(logger, event.oldObj, accessor, newNode)
	case (event.eventType == addEvent || event.eventType == updateEvent) && found:
		// handle changes in ownerReferences
		added, removed, changed := referencesDiffs(existingNode.owners, accessor.GetOwnerReferences())
		if len(added) != 0 || len(removed) != 0 || len(changed) != 0 {
			// check if the changed dependency graph unblock owners that are
			// waiting for the deletion of their dependents.
			gb.addUnblockedOwnersToDeleteQueue(logger, removed, changed)
			// update the node itself
			existingNode.owners = accessor.GetOwnerReferences()
			// Add the node to its new owners' dependent lists.
			gb.addDependentToOwners(logger, existingNode, added)
			// remove the node from the dependent list of node that are no longer in
			// the node's owners list.
			gb.removeDependentFromOwners(existingNode, removed)
		}

		if beingDeleted(accessor) {
			existingNode.markBeingDeleted()
		}
		gb.processTransitions(logger, event.oldObj, accessor, existingNode)
	case event.eventType == deleteEvent:
		if !found {
			logger.V(5).Info("item doesn't exist in the graph, this shouldn't happen",
				"item", accessor.GetUID(),
			)
			return true
		}

		removeExistingNode := true

		if event.virtual {
			// this is a virtual delete event, not one observed from an informer
			deletedIdentity := identityFromEvent(event, accessor)
			if existingNode.virtual {

				// our existing node is also virtual, we're not sure of its coordinates.
				// see if any dependents reference this owner with coordinates other than the one we got a virtual delete event for.
				if matchingDependents, nonmatchingDependents := partitionDependents(existingNode.getDependents(), deletedIdentity); len(nonmatchingDependents) > 0 {

					// some of our dependents disagree on our coordinates, so do not remove the existing virtual node from the graph
					removeExistingNode = false

					if len(matchingDependents) > 0 {
						// mark the observed deleted identity as absent
						gb.absentOwnerCache.Add(deletedIdentity)
						// attempt to delete dependents that do match the verified deleted identity
						for _, dep := range matchingDependents {
							gb.attemptToDelete.Add(dep)
						}
					}

					// if the delete event verified existingNode.identity doesn't exist...
					if existingNode.identity == deletedIdentity {
						// find an alternative identity our nonmatching dependents refer to us by
						replacementIdentity := getAlternateOwnerIdentity(nonmatchingDependents, deletedIdentity)
						if replacementIdentity != nil {
							// replace the existing virtual node with a new one with one of our other potential identities
							replacementNode := existingNode.clone()
							replacementNode.identity = *replacementIdentity
							gb.uidToNode.Write(replacementNode)
							// and add the new virtual node back to the attemptToDelete queue
							gb.attemptToDelete.AddRateLimited(replacementNode)
						}
					}
				}

			} else if existingNode.identity != deletedIdentity {
				// do not remove the existing real node from the graph based on a virtual delete event
				removeExistingNode = false

				// our existing node which was observed via informer disagrees with the virtual delete event's coordinates
				matchingDependents, _ := partitionDependents(existingNode.getDependents(), deletedIdentity)

				if len(matchingDependents) > 0 {
					// mark the observed deleted identity as absent
					gb.absentOwnerCache.Add(deletedIdentity)
					// attempt to delete dependents that do match the verified deleted identity
					for _, dep := range matchingDependents {
						gb.attemptToDelete.Add(dep)
					}
				}
			}
		}

		if removeExistingNode {
			// removeNode updates the graph
			gb.removeNode(existingNode)
			existingNode.dependentsLock.RLock()
			defer existingNode.dependentsLock.RUnlock()
			if len(existingNode.dependents) > 0 {
				gb.absentOwnerCache.Add(identityFromEvent(event, accessor))
			}
			for dep := range existingNode.dependents {
				gb.attemptToDelete.Add(dep)
			}
			for _, owner := range existingNode.owners {
				ownerNode, found := gb.uidToNode.Read(owner.UID)
				if !found || !ownerNode.isDeletingDependents() {
					continue
				}
				// this is to let attempToDeleteItem check if all the owner's
				// dependents are deleted, if so, the owner will be deleted.
				gb.attemptToDelete.Add(ownerNode)
			}
		}
	}
	return true
}
```

结合代码分析可以得知，当删除一个对象时使用了`Background`后台删除策略时，该对象因没有相关的`Finalizer`设置（只有删除策略为`Foreground`或`Orphan`时会设置相关`Finalizer`），会直接被删除，接着`GraphBuilder`会监听到该对象的delete事件，会将其`dependents`放入到`attemptToDelete`队列中去，触发`GarbageCollector`做`dependents`对象的回收删除处理。

#### insertNode

调用 `gb.insertNode` 将 `node` 加到 `uidToNode` 中，然后将该 `node` 添加到其 `owner` 的 `dependents` 中。

```go
// insertNode insert the node to gb.uidToNode; then it finds all owners as listed
// in n.owners, and adds the node to their dependents list.
func (gb *GraphBuilder) insertNode(logger klog.Logger, n *node) {
	gb.uidToNode.Write(n)
	gb.addDependentToOwners(logger, n, n.owners)
}
```

#### processTransitions

gb.processTransitions 方法检查k8s对象是否处于删除状态（对象的`deletionTimestamp`属性不为空则处于删除状态），并且对象里含有删除策略对应的`finalizer`，然后做相应的处理。

因为只有删除策略为`Foreground`或`Orphan`时对象才会会设置相关`Finalizer`，所以该方法只会处理删除策略为`Foreground`或`Orphan`的对象，对于删除策略为`Background`的对象不做处理。

若对象的`deletionTimestamp`属性不为空，且有`Orphaned`删除策略对应的`finalizer`，则将对应的`node`放入到 `attemptToOrphan` 队列中，触发`GarbageCollector`去消费处理；

若对象的`deletionTimestamp`属性不为空，且有`foreground`删除策略对应的`finalizer`，则调用`n.markDeletingDependents`标记 `node`的 `deletingDependents` 属性为 `true`，代表该`node`的`dependents`正在被删除，并将对应的`node`及其`dependents`放入到 `attemptToDelete` 队列中，触发`GarbageCollector`去消费处理。

```go
func (gb *GraphBuilder) processTransitions(logger klog.Logger, oldObj interface{}, newAccessor metav1.Object, n *node) {
	if startsWaitingForDependentsOrphaned(oldObj, newAccessor) {
		logger.V(5).Info("add item to attemptToOrphan", "item", n.identity)
		gb.attemptToOrphan.Add(n)
		return
	}
	if startsWaitingForDependentsDeleted(oldObj, newAccessor) {
		logger.V(2).Info("add item to attemptToDelete, because it's waiting for its dependents to be deleted", "item", n.identity)
		// if the n is added as a "virtual" node, its deletingDependents field is not properly set, so always set it here.
		n.markDeletingDependents()
		for dep := range n.dependents {
			gb.attemptToDelete.Add(dep)
		}
		gb.attemptToDelete.Add(n)
	}
}
```

#### removeNode

调用`gb.removeNode`，从`uidToNode`中删除该对象，然后从该`node`所有`owners`的`dependents`中删除该对象，再把该对象的`dependents`放入到`attemptToDelete`队列中，触发`GarbageCollector`处理；最后检查该 `node` 的所有 `owners`，若有处于删除状态的 `owner`，此时该 `owner` 可能处于删除阻塞状态正在等待该 `node` 的删除，将该 `owner` 加入到 `attemptToDelete`队列中，触发`GarbageCollector`处理。

```go
// removeNode removes the node from gb.uidToNode, then finds all
// owners as listed in n.owners, and removes n from their dependents list.
func (gb *GraphBuilder) removeNode(n *node) {
	gb.uidToNode.Delete(n.identity.UID)
	gb.removeDependentFromOwners(n, n.owners)
}
// removeDependentFromOwners remove n from owners' dependents list.
func (gb *GraphBuilder) removeDependentFromOwners(n *node, owners []metav1.OwnerReference) {
	for _, owner := range owners {
		ownerNode, ok := gb.uidToNode.Read(owner.UID)
		if !ok {
			continue
		}
		ownerNode.deleteDependent(n)
	}
}


```

### 2.GarbageCollector

GarbageCollector 主要有2个功能：\
（1）处理 `attemptToDelete`队列中的事件，根据对象删除策略`foreground`或`background`做相应的回收逻辑处理，删除关联对象；\
（2）处理 `attemptToOrphan`队列中的事件，根据对象删除策略`Orphan`，更新该`owner`的所有`dependents`对象，将对象的`OwnerReferences`属性中该`owner`的相关字段去除，接着再更新该`owner`对象，去除`Orphan`删除策略对应的`finalizers`。

GarbageCollector的2个关键处理方法：\
（1）`gc.runAttemptToDeleteWorker`：主要负责处理`attemptToDelete`队列中的事件，负责删除策略为`foreground`或`background`的对象回收处理；\
（2）`gc.runAttemptToOrphanWorker`：主要负责处理`attemptToOrphan`队列中的事件，负责删除策略为`Orphan`的对象回收处理。

#### 1.结构体

先来简单的分析下`GarbageCollector struct`，里面最关键的几个属性及作用如下：\
（1）`attemptToDelete`与`attemptToOrphan`：`GraphBuilder`作为生产者往`attemptToDelete` 和 `attemptToOrphan` 两个队列中存放事件，然后`GarbageCollector`作为消费者会处理 `attemptToDelete` 和 `attemptToOrphan` 两个队列中的事件。

```go
type GarbageCollector struct {
	restMapper     meta.ResettableRESTMapper
	metadataClient metadata.Interface
	// garbage collector attempts to delete the items in attemptToDelete queue when the time is ripe.
	attemptToDelete workqueue.TypedRateLimitingInterface[*node]
	// garbage collector attempts to orphan the dependents of the items in the attemptToOrphan queue, then deletes the items.
	attemptToOrphan        workqueue.TypedRateLimitingInterface[*node]
	dependencyGraphBuilder *GraphBuilder
	// GC caches the owners that do not exist according to the API server.
	absentOwnerCache *ReferenceCache

	kubeClient       clientset.Interface
	eventBroadcaster record.EventBroadcaster

	workerLock sync.RWMutex
}
```

**runAttemptToDeleteWorker**

接下来看到`GarbageCollector`的处理逻辑部分，从`gc.runAttemptToDeleteWorker`作为入口进行处理逻辑分析。

runAttemptToDeleteWorker主要逻辑为循环调用`attemptToDeleteWorker`方法。

attemptToDeleteWorker方法主要逻辑：\
（1）从`attemptToDelete`队列中取出对象；\
（2）调用 `gc.attemptToDeleteItem` 尝试删除 `node`；\
（3）若删除失败则重新加入到 `attemptToDelete` 队列中进行重试。

```go
func (gc *GarbageCollector) runAttemptToDeleteWorker(ctx context.Context) {
	for gc.processAttemptToDeleteWorker(ctx) {
	}
}

func (gc *GarbageCollector) processAttemptToDeleteWorker(ctx context.Context) bool {
	item, quit := gc.attemptToDelete.Get()
	gc.workerLock.RLock()
	defer gc.workerLock.RUnlock()
	if quit {
		return false
	}
	defer gc.attemptToDelete.Done(item)

	action := gc.attemptToDeleteWorker(ctx, item)
	switch action {
	case forgetItem:
		gc.attemptToDelete.Forget(item)
	case requeueItem:
		gc.attemptToDelete.AddRateLimited(item)
	}

	return true
}
```

**attemptToDeleteItem**

主要逻辑：\
（1）判断 `node` 是否处于删除状态；

（2）从 `apiserver` 获取该 `node` 对应的对象；

（3）调用`item.isDeletingDependents`方法：通过 `node` 的 `deletingDependents` 字段判断该 `node` 当前是否正在删除 `dependents`，若是则调用 `gc.processDeletingDependentsItem` 方法对`dependents`做进一步处理：检查该`node` 的 `blockingDependents` 是否被完全删除，若是则移除该 `node`对应对象的相关 `finalizer`，若否，则将未删除的 `blockingDependents` 加入到 `attemptToDelete`队列中；

上面分析`GraphBuilder`时说到，在 `GraphBuilder` 处理 `graphChanges` 中的事件时，在`processTransitions`方法逻辑里，会调用`n.markDeletingDependents`，标记 `node`的 `deletingDependents` 属性为 `true`；

（4）调用`gc.classifyReferences`将 `node` 的`owner`分为3类，分别是`solid`（至少有一个 `owner` 存在且不处于删除状态）、`dangling`（`owner` 均不存在）、`waitingForDependentsDeletion`（`owner` 存在，处于删除状态且正在等待其 `dependents` 被删除）；

（5）接下来将根据`solid`、`dangling`与`waitingForDependentsDeletion`的数量做不同的逻辑处理；

（6）第一种情况：当`solid`数量不为0时，即该`node`至少有一个 `owner` 存在且不处于删除状态，则说明该对象还不能被回收删除，此时将 `dangling` 和 `waitingForDependentsDeletion` 列表中的 `owner` 从 `node` 的 `ownerReferences` 中删除；

（7）第二种情况：`solid`数量为0，该 `node` 的 `owner` 处于 `waitingForDependentsDeletion` 状态并且 `node` 的 `dependents` 未被完全删除，将使用`foreground`前台删除策略来删除该`node`对应的对象；

（8）当不满足以上两种情况时（即），进入该默认处理逻辑：按照删除对象时使用的删除策略，调用 `apiserver` 的接口删除对象。

```go
func (gc *GarbageCollector) attemptToDeleteWorker(ctx context.Context, item interface{}) workQueueItemAction {
	n, ok := item.(*node)
	if !ok {
		utilruntime.HandleError(fmt.Errorf("expect *node, got %#v", item))
		return forgetItem
	}

	logger := klog.FromContext(ctx)

	if !n.isObserved() {
		nodeFromGraph, existsInGraph := gc.dependencyGraphBuilder.uidToNode.Read(n.identity.UID)
		if !existsInGraph {
			// this can happen if attemptToDelete loops on a requeued virtual node because attemptToDeleteItem returned an error,
			// and in the meantime a deletion of the real object associated with that uid was observed
			logger.V(5).Info("item no longer in the graph, skipping attemptToDeleteItem", "item", n.identity)
			return forgetItem
		}
		if nodeFromGraph.isObserved() {
			// this can happen if attemptToDelete loops on a requeued virtual node because attemptToDeleteItem returned an error,
			// and in the meantime the real object associated with that uid was observed
			logger.V(5).Info("item no longer virtual in the graph, skipping attemptToDeleteItem on virtual node", "item", n.identity)
			return forgetItem
		}
	}

	err := gc.attemptToDeleteItem(ctx, n)
	if err == enqueuedVirtualDeleteEventErr {
		// a virtual event was produced and will be handled by processGraphChanges, no need to requeue this node
		return forgetItem
	} else if err == namespacedOwnerOfClusterScopedObjectErr {
		// a cluster-scoped object referring to a namespaced owner is an error that will not resolve on retry, no need to requeue this node
		return forgetItem
	} else if err != nil {
		if _, ok := err.(*restMappingError); ok {
			// There are at least two ways this can happen:
			// 1. The reference is to an object of a custom type that has not yet been
			//    recognized by gc.restMapper (this is a transient error).
			// 2. The reference is to an invalid group/version. We don't currently
			//    have a way to distinguish this from a valid type we will recognize
			//    after the next discovery sync.
			// For now, record the error and retry.
			logger.V(5).Error(err, "error syncing item", "item", n.identity)
		} else {
			utilruntime.HandleError(fmt.Errorf("error syncing item %s: %v", n, err))
		}
		// retry if garbage collection of an object failed.
		return requeueItem
	} else if !n.isObserved() {
		// requeue if item hasn't been observed via an informer event yet.
		// otherwise a virtual node for an item added AND removed during watch reestablishment can get stuck in the graph and never removed.
		// see https://issue.k8s.io/56121
		logger.V(5).Info("item hasn't been observed via informer yet", "item", n.identity)
		return requeueItem
	}

	return forgetItem
}
```

#### processDeletingDependentsItem

主要逻辑：检查该`node` 的 `blockingDependents`（即阻塞`owner`删除的`dpendents`）是否被完全删除，若是则移除该 `node`对应对象的相关 `finalizer`（finalizer移除后，kube-apiserver会删除该对象），若否，则将未删除的 `blockingDependents` 加入到 `attemptToDelete`队列中

```go

// process item that's waiting for its dependents to be deleted
func (gc *GarbageCollector) processDeletingDependentsItem(logger klog.Logger, item *node) error {
	blockingDependents := item.blockingDependents()
	if len(blockingDependents) == 0 {
		logger.V(2).Info("remove DeleteDependents finalizer for item", "item", item.identity)
		return gc.removeFinalizer(logger, item, metav1.FinalizerDeleteDependents)
	}
	for _, dep := range blockingDependents {
		if !dep.isDeletingDependents() {
			logger.V(2).Info("adding dependent to attemptToDelete, because its owner is deletingDependents",
				"item", item.identity,
				"dependent", dep.identity,
			)
			gc.attemptToDelete.Add(dep)
		}
	}
	return nil
}
```

#### blockingDependents

item.blockingDependents返回会阻塞`node`删除的`dependents`。一个`dependents`会不会阻塞`owner`的删除，主要看这个`dependents`的`ownerReferences`的`blockOwnerDeletion`属性值是否为`true`，为`true`则代表该`dependents`会阻塞`owner`的删除。

```go
// blockingDependents returns the dependents that are blocking the deletion of
// n, i.e., the dependent that has an ownerReference pointing to n, and
// the BlockOwnerDeletion field of that ownerReference is true.
// Note that this function does not provide any synchronization guarantees;
// items could be added to or removed from ownerNode.dependents the moment this
// function returns.
func (n *node) blockingDependents() []*node {
    dependents := n.getDependents()
    var ret []*node
    for _, dep := range dependents {
       for _, owner := range dep.owners {
          if owner.UID == n.identity.UID && owner.BlockOwnerDeletion != nil && *owner.BlockOwnerDeletion {
             ret = append(ret, dep)
          }
       }
    }
    return ret
}
```

**runAttemptToOrphanWorker**

gc.runAttemptToOrphanWorker方法是负责处理`orphan`删除策略删除的 `node`。

gc.runAttemptToDeleteWorker主要逻辑为循环调用`gc.attemptToDeleteWorker`方法。

下面来看一下`gc.attemptToDeleteWorker`方法的主要逻辑：\
（1）从`attemptToOrphan`队列中取出对象；\
（2）调用`gc.orphanDependents`方法：更新该`owner`的所有`dependents`对象，将对象的`OwnerReferences`属性中该`owner`的相关字段去除，失败则将该`owner`重新加入到`attemptToOrphan`队列中；\
（3）调用`gc.removeFinalizer`方法：更新该`owner`对象，去除`Orphan`删除策略对应的`finalizers`。

```go
func (gc *GarbageCollector) runAttemptToOrphanWorker(logger klog.Logger) {
	for gc.processAttemptToOrphanWorker(logger) {
	}
}
// processAttemptToOrphanWorker dequeues a node from the attemptToOrphan, then finds its
// dependents based on the graph maintained by the GC, then removes it from the
// OwnerReferences of its dependents, and finally updates the owner to remove
// the "Orphan" finalizer. The node is added back into the attemptToOrphan if any of
// these steps fail.
func (gc *GarbageCollector) processAttemptToOrphanWorker(logger klog.Logger) bool {
	item, quit := gc.attemptToOrphan.Get()
	gc.workerLock.RLock()
	defer gc.workerLock.RUnlock()
	if quit {
		return false
	}
	defer gc.attemptToOrphan.Done(item)

	action := gc.attemptToOrphanWorker(logger, item)
	switch action {
	case forgetItem:
		gc.attemptToOrphan.Forget(item)
	case requeueItem:
		gc.attemptToOrphan.AddRateLimited(item)
	}

	return true
}
```

**orphanDependents**

主要逻辑：更新指定`owner`的所有`dependents`对象，将对象的`OwnerReferences`属性中该`owner`的相关字段去除，对于每个`dependents`，分别起一个goroutine来处理，加快处理速度。

```go

// dependents are copies of pointers to the owner's dependents, they don't need to be locked.
func (gc *GarbageCollector) orphanDependents(logger klog.Logger, owner objectReference, dependents []*node) error {
	errCh := make(chan error, len(dependents))
	wg := sync.WaitGroup{}
	wg.Add(len(dependents))
	for i := range dependents {
		go func(dependent *node) {
			defer wg.Done()
			// the dependent.identity.UID is used as precondition
			p, err := c.GenerateDeleteOwnerRefStrategicMergeBytes(dependent.identity.UID, []types.UID{owner.UID})
			if err != nil {
				errCh <- fmt.Errorf("orphaning %s failed, %v", dependent.identity, err)
				return
			}
			_, err = gc.patch(dependent, p, func(n *node) ([]byte, error) {
				return gc.deleteOwnerRefJSONMergePatch(n, owner.UID)
			})
			// note that if the target ownerReference doesn't exist in the
			// dependent, strategic merge patch will NOT return an error.
			if err != nil && !errors.IsNotFound(err) {
				errCh <- fmt.Errorf("orphaning %s failed, %v", dependent.identity, err)
			}
		}(dependents[i])
	}
	wg.Wait()
	close(errCh)

	var errorsSlice []error
	for e := range errCh {
		errorsSlice = append(errorsSlice, e)
	}

	if len(errorsSlice) != 0 {
		return fmt.Errorf("failed to orphan dependents of owner %s, got errors: %s", owner, utilerrors.NewAggregate(errorsSlice).Error())
	}
	logger.V(5).Info("successfully updated all dependents", "owner", owner)
	return nil
}

```
